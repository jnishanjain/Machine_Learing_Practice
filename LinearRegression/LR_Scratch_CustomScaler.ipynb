{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "EDfcSmn4QafJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We are creating really high values so we can show case how its harder for Gradient decent to\n",
        "np.random.seed(59)\n",
        "X = np.random.randint(0,1000000,(50000,5))\n",
        "weights = np.array([-69,4,28,67,74])\n",
        "bias = 987\n",
        "y = np.dot(X,weights) + bias\n"
      ],
      "metadata": {
        "id": "SZ7uRU5gFJNU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking our data\n",
        "print(X.shape)\n",
        "print(X[:5,:])\n",
        "print(y.shape)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "BP9KUhI-R-XB",
        "outputId": "5c656151-17d4-437b-f7a7-f41acb5115ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 5)\n",
            "[[889009 744876 668877 918167 927083]\n",
            " [678095 741173 645177 375168 911199]\n",
            " [865347 119104 760403 535031  66861]\n",
            " [ 33758 415705 696752 787273 229012]\n",
            " [286437 998466 368246 334113 857291]]\n",
            "(50000,)\n",
            "[ 90488757  66807062   2854535 ...  16161166 115690186  83172750]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LineraRegression:\n",
        "  def __init__(self,learning_rate = 0.1,max_iterations=10000):\n",
        "    self.lr = learning_rate\n",
        "    self.max_itr = max_iterations\n",
        "    self.weights = None\n",
        "    self.bias = None\n",
        "    self.loss_history = []\n",
        "\n",
        "  def fit(self,X,y):\n",
        "    #lets initialize weights first\n",
        "    n_samples , n_features = X.shape\n",
        "    self.weights = np.zeros(n_features)\n",
        "    self.bias = 0\n",
        "    for epoch in range(self.max_itr):\n",
        "      y_pred = self.predict(X)\n",
        "      error = y_pred - y\n",
        "      dw = 2/n_samples * np.dot(X.T,error)\n",
        "      db = 2/n_samples * np.sum(error)\n",
        "      self.weights = self.weights - self.lr*dw\n",
        "      self.bias = self.bias -self.lr*db\n",
        "      loss = np.mean(error**2)\n",
        "      self.loss_history.append(np.mean(error**2))\n",
        "      if epoch % 1000 == 0 :\n",
        "        print(f\"Epoch {epoch} , Loss {loss}\")\n",
        "  def predict(self,X):\n",
        "    return np.dot(X,self.weights) + self.bias\n"
      ],
      "metadata": {
        "id": "8FcTA7YF_j_C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LineraRegression(learning_rate=0.00001)\n",
        "model.fit(X,y)"
      ],
      "metadata": {
        "id": "uFJ0ez5WST7-",
        "outputId": "cd5247ab-ad2b-404b-9ba9-985a1496ea3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 , Loss 3986933616990935.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-688738749.py:21: RuntimeWarning: overflow encountered in square\n",
            "  loss = np.mean(error**2)\n",
            "/tmp/ipython-input-688738749.py:22: RuntimeWarning: overflow encountered in square\n",
            "  self.loss_history.append(np.mean(error**2))\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-688738749.py:19: RuntimeWarning: invalid value encountered in subtract\n",
            "  self.weights = self.weights - self.lr*dw\n",
            "/tmp/ipython-input-688738749.py:20: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  self.bias = self.bias -self.lr*db\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1000 , Loss nan\n",
            "Epoch 2000 , Loss nan\n",
            "Epoch 3000 , Loss nan\n",
            "Epoch 4000 , Loss nan\n",
            "Epoch 5000 , Loss nan\n",
            "Epoch 6000 , Loss nan\n",
            "Epoch 7000 , Loss nan\n",
            "Epoch 8000 , Loss nan\n",
            "Epoch 9000 , Loss nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see our data vales are too high, and python throws as overflow error because our values become too large, there is a solution for this which is Scaling, so now we will scale out data using a StandardScaler method."
      ],
      "metadata": {
        "id": "lRqpy4BaTyvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomScalar:\n",
        "  def __init__(self,):\n",
        "    self.mean = None\n",
        "    self.std = None\n",
        "\n",
        "  def fit_transform(self,X):\n",
        "    self.mean = np.mean(X,axis=0)\n",
        "    self.std = np.std(X,axis=0)\n",
        "    return (X-self.mean)/self.std\n",
        "  def transform(self,X):\n",
        "    return (X-self.mean)/self.std\n",
        "  def inverse_transform(self,X):\n",
        "    return X*self.std + self.mean\n"
      ],
      "metadata": {
        "id": "F8quEOXQSfSg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = CustomScalar()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "y_scaled = scaler.fit_transform(y)"
      ],
      "metadata": {
        "id": "Uw3irQ15wEs5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_scaled[:5]"
      ],
      "metadata": {
        "id": "QW9v0uKBwXOu",
        "outputId": "2f8711be-77a9-48f8-ed04-455e0209a867",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.07385005,  0.41390836, -1.36826722,  1.01948074,  0.79177677])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_scaled[:5]"
      ],
      "metadata": {
        "id": "QoLNrXGbwqiM",
        "outputId": "0a24b077-db7b-4a6b-bd29-3ce5cce5f8a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.35651625,  0.84745065,  0.58474295,  1.45888068,  1.48389148],\n",
              "       [ 0.62587627,  0.83466261,  0.50258458, -0.42385395,  1.42879815],\n",
              "       [ 1.27454728, -1.31360603,  0.90202676,  0.13043732, -1.49977121],\n",
              "       [-1.60621063, -0.28931672,  0.68137436,  1.00503331, -0.93735373],\n",
              "       [-0.73088997,  1.72320469, -0.45742376, -0.56620351,  1.24181934]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LineraRegression(learning_rate=0.01)\n",
        "model.fit(X_scaled,y_scaled)"
      ],
      "metadata": {
        "id": "jcqKK9Pp_pK2",
        "outputId": "6bf010f1-fb8a-4893-faa1-960730eeee62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 , Loss 0.9999999999999997\n",
            "Epoch 1000 , Loss 3.4248343416107516e-18\n",
            "Epoch 2000 , Loss 2.2788417493042975e-29\n",
            "Epoch 3000 , Loss 2.2788417512041916e-29\n",
            "Epoch 4000 , Loss 2.2788417541556174e-29\n",
            "Epoch 5000 , Loss 2.2788417539868697e-29\n",
            "Epoch 6000 , Loss 2.2789471643599426e-29\n",
            "Epoch 7000 , Loss 2.2789471673537882e-29\n",
            "Epoch 8000 , Loss 2.2789471671504354e-29\n",
            "Epoch 9000 , Loss 2.278947168369379e-29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Orignal Weights : {weights}\")\n",
        "print(f\"Predicted Weights : {model.weights}\")\n",
        "print(f\"Orignal Bias : {bias}\")\n",
        "print(f\"Predicted Bias : {model.bias}\")"
      ],
      "metadata": {
        "id": "hVtIzv9E_wQg",
        "outputId": "27226bc5-af2f-4797-d6e6-0e7edca71706",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orignal Weights : [-69   4  28  67  74]\n",
            "Predicted Weights : [-0.555065    0.03227771  0.22508543  0.53848982  0.5945458 ]\n",
            "Orignal Bias : 987\n",
            "Predicted Bias : 1.6687272810176308e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is working fine, the reason why the weights and bias dont match is because we have scaled the data, so here we should look at weights and bias directly, but instead at the prediction"
      ],
      "metadata": {
        "id": "1zMQSvsHDswO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SBfsrdLDDWZX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}